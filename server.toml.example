listen = "tcp://*:25566"
world = "worlds/generic-world-name"

# params passed to the model are defined in their own section below
# context_length is the context window of the model.
context_length = 30000

# max_tokens limits the text generation
max_tokens = 300

loglevel = "warning"

# There is no good reason not to use the default embedding.
# If you change embedding, the embeddings of your previous data will be unreadable
# Leaving unset will use Sentence Transformers,
# which uses pytorch, so will use CUDA if available.
# For local, it depends, but all-MiniLM-L6-v2 is probably sufficient (the default)
#embedding_provider = "OpenAI"
#embedding = "text-embedding-3-small"
embedding = "all-MiniLM-L6-v2"

# The engine will use [providers.backend] for generating places, character, plot etc.
# This one should be good at instruction following, but does not have to be a great writer.
[providers.backend]
api_base = "http://localhost:5000/v1"
api_key = "your-api-key"

# The engine will use [providers.narrator] for generating the narrative prose.
[providers.narrator]
api_base = "http://localhost:5001/v1"
api_key = "your-api-key"

# example local provider format using tabbyAPI
[providers.tabbyapi]
api_base = "http://localhost:5002/v1"
api_key = "your-api-key"

# example OpenAI-compatible (e.g. OpenAI, deepinfra) provider format
[providers.OpenAI]
model = "gpt-4o-mini"
api_base = "https://api.openai.com/v1"
api_key = "sk-your-openai-key"

# example Anthropic provider format
[providers.anthropic]
api_scheme = "anthropic"
api_key = "sk-your-anthropic-key"
model = "claude-3-5-sonnet-20240620"
